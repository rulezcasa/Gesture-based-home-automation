{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29cc6d7-8a73-40cc-a1c5-b528a6555f62",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd78393-1bdc-48db-90e4-0b595ed09f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093f6860-85d3-4cb7-813a-6cdf528e960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08483ed9-6b4d-4741-b942-699ae448116d",
   "metadata": {},
   "source": [
    "## Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34cc82d6-c7cb-4701-aff6-d4ae396d062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "\n",
    "# Global variables to calculate FPS\n",
    "COUNTER, FPS = 0, 0\n",
    "START_TIME = time.time()\n",
    "recognition_result_list = []\n",
    "\n",
    "#Tracking detections\n",
    "appliance_signs = ['fan', 'light']\n",
    "state_signs=['on','off']\n",
    "appliance_detected = False\n",
    "state_detected = False\n",
    "appliance = None\n",
    "state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "618dfdf8-1649-4811-9b9a-e40656607490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def appliance_state_callback(category_name):\n",
    "    global appliance_detected, state_detected, appliance, state\n",
    "    if category_name in appliance_signs:\n",
    "        appliance_detected = True\n",
    "        appliance = category_name\n",
    "    \n",
    "    if category_name in state_signs:\n",
    "        state_detected = True\n",
    "        state = category_name\n",
    "\n",
    "    if appliance_detected==True and state_detected==True:\n",
    "        print(f\"Turning {appliance} {state}\") #control signal to hardware\n",
    "        appliance_detected = False\n",
    "        state_detected = False\n",
    "        appliance = None\n",
    "        state = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc1268c-131a-49ec-a7b4-3454010d83a9",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf67201-046c-4af9-8723-20fda995b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    model: str,\n",
    "    num_hands: int,\n",
    "    min_hand_detection_confidence: float,\n",
    "    min_hand_presence_confidence: float,\n",
    "    min_tracking_confidence: float,\n",
    "    camera_id: int,\n",
    "    width: int,\n",
    "    height: int,\n",
    ") -> None:\n",
    "\n",
    "    # Start capturing video input from the camera\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "    # Visualization parameters\n",
    "    row_size = 50  # pixels\n",
    "    left_margin = 24  # pixels\n",
    "    text_color = (0, 0, 0)  # black\n",
    "    font_size = 1\n",
    "    font_thickness = 1\n",
    "    fps_avg_frame_count = 10\n",
    "\n",
    "    # Label box parameters\n",
    "    label_text_color = (0, 0, 0)  # white\n",
    "    label_font_size = 1\n",
    "    label_thickness = 2\n",
    "\n",
    "    recognition_frame = None\n",
    "\n",
    "    def save_result(\n",
    "        result: vision.GestureRecognizerResult,\n",
    "        unused_output_image: mp.Image,\n",
    "        timestamp_ms: int,\n",
    "    ):\n",
    "\n",
    "        global FPS, COUNTER, START_TIME\n",
    "\n",
    "        \n",
    "        # Calculate the FPS\n",
    "        if COUNTER % fps_avg_frame_count == 0:\n",
    "            FPS = fps_avg_frame_count / (time.time() - START_TIME)\n",
    "            START_TIME = time.time()\n",
    "\n",
    "        recognition_result_list.append(result)\n",
    "        COUNTER += 1\n",
    "        \n",
    "        if result.gestures:\n",
    "            gesture = result.gestures[0]\n",
    "            category_name = gesture[0].category_name.lower()\n",
    "            appliance_state_callback(category_name)\n",
    "        \n",
    "\n",
    "    # Initialize the gesture recognizer model\n",
    "    base_options = python.BaseOptions(model_asset_path=model)\n",
    "    options = vision.GestureRecognizerOptions(\n",
    "        base_options=base_options,\n",
    "        running_mode=vision.RunningMode.LIVE_STREAM,\n",
    "        num_hands=num_hands,\n",
    "        min_hand_detection_confidence=min_hand_detection_confidence,\n",
    "        min_hand_presence_confidence=min_hand_presence_confidence,\n",
    "        min_tracking_confidence=min_tracking_confidence,\n",
    "        result_callback=save_result,\n",
    "    )\n",
    "    recognizer = vision.GestureRecognizer.create_from_options(options)\n",
    "\n",
    "    # Continuously capture images from the camera and run inference\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            sys.exit(\n",
    "                \"ERROR: Unable to read from webcam. Please verify your webcam settings.\"\n",
    "            )\n",
    "\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "        # Convert the image from BGR to RGB as required by the TFLite model.\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)\n",
    "\n",
    "        # Run gesture recognizer using the model.\n",
    "        recognizer.recognize_async(mp_image, time.time_ns() // 1_000_000)\n",
    "\n",
    "        # Show the FPS\n",
    "        fps_text = \"FPS = {:.1f}\".format(FPS)\n",
    "        text_location = (left_margin, row_size)\n",
    "        current_frame = image\n",
    "        cv2.putText(\n",
    "            current_frame,\n",
    "            fps_text,\n",
    "            text_location,\n",
    "            cv2.FONT_HERSHEY_DUPLEX,\n",
    "            font_size,\n",
    "            text_color,\n",
    "            font_thickness,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "        if recognition_result_list:\n",
    "            # Draw landmarks and write the text for each hand.\n",
    "            for hand_index, hand_landmarks in enumerate(\n",
    "                recognition_result_list[0].hand_landmarks\n",
    "            ):\n",
    "                # Calculate the bounding box of the hand\n",
    "                x_min = min([landmark.x for landmark in hand_landmarks])\n",
    "                y_min = min([landmark.y for landmark in hand_landmarks])\n",
    "                y_max = max([landmark.y for landmark in hand_landmarks])\n",
    "\n",
    "                # Convert normalized coordinates to pixel values\n",
    "                frame_height, frame_width = current_frame.shape[:2]\n",
    "                x_min_px = int(x_min * frame_width)\n",
    "                y_min_px = int(y_min * frame_height)\n",
    "                y_max_px = int(y_max * frame_height)\n",
    "\n",
    "                # Get gesture classification results\n",
    "                if recognition_result_list[0].gestures:\n",
    "                    gesture = recognition_result_list[0].gestures[hand_index]\n",
    "                    category_name = gesture[0].category_name\n",
    "                    score = round(gesture[0].score, 2)\n",
    "                    result_text = f\"{category_name} ({score})\"\n",
    "\n",
    "                    # Compute text size\n",
    "                    text_size = cv2.getTextSize(\n",
    "                        result_text,\n",
    "                        cv2.FONT_HERSHEY_DUPLEX,\n",
    "                        label_font_size,\n",
    "                        label_thickness,\n",
    "                    )[0]\n",
    "                    text_width, text_height = text_size\n",
    "\n",
    "                    # Calculate text position (above the hand)\n",
    "                    text_x = x_min_px\n",
    "                    text_y = y_min_px - 10  # Adjust this value as needed\n",
    "\n",
    "                    # Make sure the text is within the frame boundaries\n",
    "                    if text_y < 0:\n",
    "                        text_y = y_max_px + text_height\n",
    "\n",
    "                    # Draw the text\n",
    "                    cv2.putText(\n",
    "                        current_frame,\n",
    "                        result_text,\n",
    "                        (text_x, text_y),\n",
    "                        cv2.FONT_HERSHEY_DUPLEX,\n",
    "                        label_font_size,\n",
    "                        label_text_color,\n",
    "                        label_thickness,\n",
    "                        cv2.LINE_AA,\n",
    "                    )\n",
    "\n",
    "                # Draw hand landmarks on the frame\n",
    "                hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "                hand_landmarks_proto.landmark.extend(\n",
    "                    [\n",
    "                        landmark_pb2.NormalizedLandmark(\n",
    "                            x=landmark.x, y=landmark.y, z=landmark.z\n",
    "                        )\n",
    "                        for landmark in hand_landmarks\n",
    "                    ]\n",
    "                )\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    current_frame,\n",
    "                    hand_landmarks_proto,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style(),\n",
    "                )\n",
    "\n",
    "            recognition_frame = current_frame\n",
    "            recognition_result_list.clear()\n",
    "\n",
    "        if recognition_frame is not None:\n",
    "            cv2.imshow(\"gesture_recognition\", recognition_frame)\n",
    "\n",
    "        # Stop the program if the ESC key is pressed.\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "\n",
    "    recognizer.close()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8e99e7-5c2d-4b8e-bf90-f86602e157c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = 'gesture_recognizer2.task'\n",
    "    num_hands = 1\n",
    "    min_hand_detection_confidence = 0.5\n",
    "    min_hand_presence_confidence = 0.5\n",
    "    min_tracking_confidence = 0.5\n",
    "    camera_id = 0\n",
    "    frame_width = 640\n",
    "    frame_height = 480\n",
    "\n",
    "    run(model, num_hands, min_hand_detection_confidence,\n",
    "        min_hand_presence_confidence, min_tracking_confidence,\n",
    "        camera_id, frame_width, frame_height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebed619f-8fbb-41d7-a1c0-66eee160ba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 17:43:33.865 python3.11[32479:2727798] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1711455215.500182       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1 Pro\n",
      "W0000 00:00:1711455215.500464       1 gesture_recognizer_graph.cc:129] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleration to Xnnpack.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning light on\n",
      "Turning light on\n",
      "Turning light on\n",
      "Turning light on\n",
      "Turning light off\n",
      "Turning fan off\n"
     ]
    }
   ],
   "source": [
    "# Call the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b98fb-b2dd-44c6-9259-26080a8f5729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd254118-5ee2-450c-a1a4-9f774d922559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
